{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5180b-2599-4da0-9bb2-2e83f0036f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The task is to create a system that will match your uploaded resume to a posted job link and calculate how much your profile matches the\n",
    "job description and give your percentage and also gives you suggestions on your resume to increase the match percentage. \n",
    "\n",
    "Steps needed:\n",
    "1. Setup openAI key to access some LLM models ✅\n",
    "2. Parse the job website successfully \n",
    "    - Making sure it is a job website \n",
    "    - Remove unecessary tags from the website \n",
    "    - Extract contents like requirements, qualification, responsibilities... from the website and convert it to a class\n",
    "3. Parse the uploaded resume \n",
    "    - Make sure the uploaded file is actually a resume\n",
    "    - Extract contents like education, work_experience, skills, certificates.... from the resume and convert it to a class\n",
    "4. ==== have some logic to compare the match of resume and job description\n",
    "5. ==== give suggestions to make it more fit for the job and show the updated matching percentage\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd22a7b-d596-49f5-a280-a23de63987f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2129621-1537-4660-8231-e6d616f1d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load openAI key and check if the key is valid\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI(api_key=api_key)\n",
    "\n",
    "def check_openai_api_key(api_key):\n",
    "    try:\n",
    "        openai.models.list()\n",
    "    except:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "check_openai_api_key(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ccd72-1426-4234-ba2b-8c3a80225c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample conversation to check if we have a working api \n",
    "message = \"Hello, GPT! This is my first ever message to you! Hi!\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3553f6-aa97-478f-bed9-97ac66ca8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a jobwebsite object that will have the necessary actions and attributes\n",
    "class JobWebsite:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.raw_html = None\n",
    "        self.title = None\n",
    "        self.company = None\n",
    "        self.location = None\n",
    "        self.description = None\n",
    "        self.requirements = []\n",
    "        self.qualifications = []\n",
    "        self.responsibilities = []\n",
    "\n",
    "        self._parse()\n",
    "\n",
    "    def _parse(self):\n",
    "        resp = requests.get(self.url)\n",
    "        self.raw_html = resp.text\n",
    "        soup = BeautifulSoup(self.raw_html, 'html.parser')\n",
    "\n",
    "        # Sanity check for job-related keywords\n",
    "        # print(\"Soup\",soup.prettify())\n",
    "        text = soup.get_text(separator=' ').lower()\n",
    "        print(\"TExtt\",text)\n",
    "        if not any(k in text for k in ('responsibilities', 'qualifications', 'requirements')):\n",
    "            raise ValueError(\"URL does not look like a job posting: {}\".format(self.url))\n",
    "\n",
    "        # Adjust these selectors to suit your target site\n",
    "        title_node = soup.find('h1')\n",
    "        company_node = soup.find(class_='company')\n",
    "        location_node = soup.find(class_='location')\n",
    "        desc_node = soup.find(class_='job-description')\n",
    "\n",
    "        def extract_list(class_name):\n",
    "            section = soup.find(class_=class_name)\n",
    "            if not section:\n",
    "                return []\n",
    "            return [li.get_text(strip=True) for li in section.find_all('li')]\n",
    "\n",
    "        self.title = title_node.get_text(strip=True) if title_node else None\n",
    "        self.company = company_node.get_text(strip=True) if company_node else None\n",
    "        self.location = location_node.get_text(strip=True) if location_node else None\n",
    "        self.description = desc_node.get_text(separator=' ', strip=True) if desc_node else None\n",
    "\n",
    "        self.requirements = extract_list('requirements')\n",
    "        self.qualifications = extract_list('qualifications')\n",
    "        self.responsibilities = extract_list('responsibilities')\n",
    "    def get_parsed_info(self):\n",
    "        return {\n",
    "            'url':            self.url,\n",
    "            'title':          title_node.get_text(strip=True) if title_node else None,\n",
    "            'company':        company_node.get_text(strip=True) if company_node else None,\n",
    "            'location':       location_node.get_text(strip=True) if location_node else None,\n",
    "            'description':    desc_node.get_text(separator=' ', strip=True) if desc_node else None,\n",
    "            'requirements':   extract_list('requirements'),\n",
    "            'qualifications': extract_list('qualifications'),\n",
    "            'responsibilities': extract_list('responsibilities'),\n",
    "            'raw_html':       html\n",
    "        }\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b1699-7b5f-4eee-8907-d782d09756d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_job = JobWebsite('https://edwarddonner.com')\n",
    "sample_job = JobWebsite('https://jobs.micro1.ai/post/86d3f0f5-2f68-42ff-9a3c-59ccf43ccb2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9be7a-496c-4b4f-97fa-28fa473ad2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resume:\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Initialize with a URL or local path to a PDF resume.\n",
    "        Downloads (if remote), extracts all text, then parses\n",
    "        out common sections into lists of lines.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.raw_text = self._load_pdf_text()\n",
    "        self.education = []\n",
    "        self.work_experience = []\n",
    "        self.certificates = []\n",
    "        self.skills = []\n",
    "        self.other_sections = {}  # catch-all for any additional headings\n",
    "        self._parse_sections()\n",
    "\n",
    "    def _load_pdf_text(self):\n",
    "        # Fetch PDF bytes\n",
    "        if self.url.startswith(('http://', 'https://')):\n",
    "            resp = requests.get(self.url)\n",
    "            resp.raise_for_status()\n",
    "            pdf_stream = io.BytesIO(resp.content)\n",
    "        else:\n",
    "            pdf_stream = open(self.url, 'rb')\n",
    "\n",
    "        # Extract text from each page\n",
    "        reader = PdfReader(pdf_stream)\n",
    "        text = []\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text.append(page_text)\n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    def _parse_sections(self):\n",
    "        # Define the section headers we care about (case-insensitive)\n",
    "        headers = {\n",
    "            \"education\":      r\"education\",\n",
    "            \"work_experience\":r\"(work experience|professional experience|experience)\",\n",
    "            \"certificates\":   r\"(certificates|certifications)\",\n",
    "            \"skills\":         r\"skills\",\n",
    "        }\n",
    "        # Build a regex that matches any of these headers\n",
    "        header_regex = re.compile(\n",
    "            r\"^(%s)\\s*$\" % \"|\".join(v for v in headers.values()),\n",
    "            flags=re.IGNORECASE | re.MULTILINE\n",
    "        )\n",
    "\n",
    "        # Find all header positions\n",
    "        matches = list(header_regex.finditer(self.raw_text))\n",
    "        # Append a dummy match at end to capture last section\n",
    "        end_of_doc = re.Match  # placeholder\n",
    "        matches.append(re.match(r\"^$\", \"\"))  # zero-length at end\n",
    "\n",
    "        for i in range(len(matches) - 1):\n",
    "            header_text = matches[i].group(1).strip().lower()\n",
    "            start = matches[i].end()\n",
    "            end = matches[i+1].start()\n",
    "            section_lines = [\n",
    "                line.strip() for line in\n",
    "                self.raw_text[start:end].splitlines()\n",
    "                if line.strip()\n",
    "            ]\n",
    "\n",
    "            # Map header_text back to our attribute names\n",
    "            for attr, pattern in headers.items():\n",
    "                if re.fullmatch(pattern, header_text, flags=re.IGNORECASE):\n",
    "                    setattr(self, attr, section_lines)\n",
    "                    break\n",
    "            else:\n",
    "                # anything else goes to other_sections\n",
    "                self.other_sections[header_text] = section_lines\n",
    "\n",
    "    def as_dict(self):\n",
    "        \"\"\"\n",
    "        Return all parsed content as a dict.\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"url\": self.url,\n",
    "            \"education\": self.education,\n",
    "            \"work_experience\": self.work_experience,\n",
    "            \"certificates\": self.certificates,\n",
    "            \"skills\": self.skills,\n",
    "        }\n",
    "        data.update(self.other_sections)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b715986-25f6-4e6d-8d4b-af7e27199c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume = Resume(\"https://drive.google.com/file/d/1qtSwz3WcG2t2unuggYTUx2oitxbpYJSD/view?usp=sharing\")\n",
    "resume = Resume(\"https://drive.usercontent.google.com/u/0/uc?id=1qtSwz3WcG2t2unuggYTUx2oitxbpYJSD&export=download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8929c73-d7c3-4bd7-a765-599b4f984ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_resume(resume):\n",
    "    \"\"\"\n",
    "    Summarize a Resume instance by sending all of its parsed sections\n",
    "    to OpenAI in one shot. Returns a dict parsed from the JSON that\n",
    "    the model generates.\n",
    "    \"\"\"\n",
    "    # 1) Extract everything from the Resume object\n",
    "    # ------------------------------------------------\n",
    "    sections: Dict[str, Any] = {\n",
    "        \"education\": resume.education,\n",
    "        \"work_experience\": resume.work_experience,\n",
    "        \"certificates\": resume.certificates,\n",
    "        \"skills\": resume.skills,\n",
    "    }\n",
    "    # Include any extra headings that were captured:\n",
    "    sections.update(resume.other_sections)\n",
    "\n",
    "    # 2) Build a single prompt string that includes all section data.\n",
    "    # ------------------------------------------------\n",
    "    #    We label each section, then dump the Python lists/dicts as JSON\n",
    "    #    inside the prompt so the model can “see” the raw content exactly.\n",
    "    serialized_sections = json.dumps(sections, indent=2)\n",
    "    prompt = (\n",
    "        \"You are a resume‐parsing assistant. You will receive a JSON object whose keys are section names\\n\"\n",
    "        \"(e.g. \\\"education\\\", \\\"work_experience\\\", \\\"certificates\\\", \\\"skills\\\", plus any other headings the parser found)\\n\"\n",
    "        \"and whose values are lists of lines (strings) extracted from that section. Some sections may be empty lists.\\n\\n\"\n",
    "        \"Your job:\\n\"\n",
    "        \"1. For each of the known keys:\\n\"\n",
    "        \"   - “education”: produce a cleaned‐up list of educational entries (e.g. school, degree, dates).\\n\"\n",
    "        \"   - “work_experience”: produce a cleaned‐up list of job entries (e.g. company, title, dates, bullets).\\n\"\n",
    "        \"   - “certificates”: produce a cleaned‐up list of certification names (and issuing organizations, if available).\\n\"\n",
    "        \"   - “skills”: produce a cleaned‐up list of skills (e.g. “Python”, “Project Management”, etc.).\\n\"\n",
    "        \"2. For any additional key (in “other_sections”), keep the title as‐is and produce a cleaned list of lines under it.\\n\"\n",
    "        \"3. Return exactly one JSON object. The top‐level keys must match the input section names. Under each key,\\n\"\n",
    "        \"   supply an array of objects or strings as appropriate. For example:\\n\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"education\\\": [\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"      \\\"school\\\": \\\"University of X\\\",\\n\"\n",
    "        \"      \\\"degree\\\": \\\"B.Sc. Computer Science\\\",\\n\"\n",
    "        \"      \\\"dates\\\": \\\"2015 – 2019\\\"\\n\"\n",
    "        \"    },\\n\"\n",
    "        \"    …\\n\"\n",
    "        \"  ],\\n\"\n",
    "        \"  \\\"work_experience\\\": [\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"      \\\"company\\\": \\\"Acme Corp\\\",\\n\"\n",
    "        \"      \\\"title\\\": \\\"Software Engineer\\\",\\n\"\n",
    "        \"      \\\"dates\\\": \\\"Jan 2020 – Present\\\",\\n\"\n",
    "        \"      \\\"details\\\": [\\n\"\n",
    "        \"        \\\"Built REST APIs in Python that served 1M+ users\\\",\\n\"\n",
    "        \"        \\\"Led a team of 3 engineers on feature XYZ\\\"\\n\"\n",
    "        \"      ]\\n\"\n",
    "        \"    },\\n\"\n",
    "        \"    …\\n\"\n",
    "        \"  ],\\n\"\n",
    "        \"  \\\"certificates\\\": [\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"      \\\"name\\\": \\\"AWS Certified Solutions Architect\\\",\\n\"\n",
    "        \"      \\\"issuer\\\": \\\"Amazon Web Services\\\",\\n\"\n",
    "        \"      \\\"date\\\": \\\"2021\\\"\\n\"\n",
    "        \"    },\\n\"\n",
    "        \"    …\\n\"\n",
    "        \"  ],\\n\"\n",
    "        \"  \\\"skills\\\": [\\n\"\n",
    "        \"    \\\"Python\\\",\\n\"\n",
    "        \"    \\\"Docker\\\",\\n\"\n",
    "        \"    \\\"Project Management\\\",\\n\"\n",
    "        \"    …\\n\"\n",
    "        \"  ],\\n\"\n",
    "        \"  \\\"<other_heading_1>\\\": [\\n\"\n",
    "        \"    \\\"…\\\",\\n\"\n",
    "        \"    \\\"…\\\"\\n\"\n",
    "        \"  ],\\n\"\n",
    "        \"  \\\"<other_heading_2>\\\": [\\n\"\n",
    "        \"    \\\"…\\\"\\n\"\n",
    "        \"  ]\\n\"\n",
    "        \"}\\n\\n\"\n",
    "        \"Don’t include any explanations—just raw JSON. Here is the raw extracted‐sections input:\\n\\n\"\n",
    "        \"```json\\n\"\n",
    "        + serialized_sections\n",
    "        + \"\\n```\"\n",
    "    )\n",
    "\n",
    "    # 3) Call OpenAI’s chat completion endpoint (single request).\n",
    "    # ------------------------------------------------\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",           \n",
    "        messages=[\n",
    "            {\"role\": \"system\",  \"content\": \"You are a helpful assistant specialized in structuring resume data.\"},\n",
    "            {\"role\": \"user\",    \"content\": prompt.strip()},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 4) Extract the assistant’s JSON string from the response,\n",
    "    #    parse it back into a Python dict, and return.\n",
    "    # ------------------------------------------------\n",
    "    raw_output = response.choices[0].message.content.strip()\n",
    "    return raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c867dd-8d1d-4167-838d-948ccb555df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_resume = summarize_resume(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c5664-4773-4532-b4a7-905928663fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaec699-1af4-4ea5-a171-e7cc3c817411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
